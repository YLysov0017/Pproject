{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fde72582-41e0-4c8c-95a2-5ddbd267d488",
      "metadata": {
        "id": "fde72582-41e0-4c8c-95a2-5ddbd267d488"
      },
      "source": [
        "# Лабараторная работа 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код парсера находится на GitHub рядом с task1.ipynb - sg_parser.py\n"
      ],
      "metadata": {
        "id": "6HRC2dq_OJP2"
      },
      "id": "6HRC2dq_OJP2"
    },
    {
      "cell_type": "markdown",
      "id": "707fd135-0fc5-4c57-9f35-85641dae7575",
      "metadata": {
        "id": "707fd135-0fc5-4c57-9f35-85641dae7575"
      },
      "source": [
        "## Часть 2 - анализ текстовых данных"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3"
      ],
      "metadata": {
        "id": "_t-L275MOi85"
      },
      "id": "_t-L275MOi85",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('/content/articles.db')\n",
        "CURSOS = conn.cursor()\n",
        "CURSOS.execute('''SELECT title, body FROM articles''')\n",
        "rows = CURSOS.fetchall()\n",
        "rows"
      ],
      "metadata": {
        "id": "zE-taniQOmX-"
      },
      "id": "zE-taniQOmX-",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows"
      ],
      "metadata": {
        "id": "kqUi7nxqPOq3"
      },
      "id": "kqUi7nxqPOq3",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "482571a2-f872-4b71-a62b-56e5032e094c",
      "metadata": {
        "id": "482571a2-f872-4b71-a62b-56e5032e094c"
      },
      "source": [
        "Провести аналитику текстовых данных.\n",
        "\n",
        "1. Токенезировать и лемматизировать полученные текстовые данные полученные из 1 части задания\n",
        "\n",
        "- Токенизация - разделение слов на предложения\n",
        "- Лемматизация - перевод слов в начальную форму\n",
        "\n",
        "2. Посчитать наиболее встречающиеся пары подлежащих и сказуемых\n",
        "3. Посчитать самые популлярные слова (исключая стоп-слова, словарь стоп слов можно найти в nltk, либо отбросить по частям речи)\n",
        "4. Вывести статистику (по убыванию, наиболее встречающеся пары и самые популярные слова за исключением стоп-слов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837f2000-52cf-4cea-85f1-7e9c5e2b0686",
      "metadata": {
        "id": "837f2000-52cf-4cea-85f1-7e9c5e2b0686"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    {'text': 'Эти типы стали есть в цеху. Пример токенизации. Для примера мы токенезируем'},\n",
        "    {'text': 'Научный советник рассказал о... (читать продолжение в источнике).'},\n",
        "    {'text': 'В статье говориться о котиках.'},\n",
        "    {'text': 'Стекло бьется.'}\n",
        "] # p.s. здесь вы открываете полученные данные\n",
        "\n",
        "# Для примера можно использовать natasha\n",
        "# !pip install natasha\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f58935-d3fc-4be0-b304-c183e32e2a3a",
      "metadata": {
        "id": "04f58935-d3fc-4be0-b304-c183e32e2a3a",
        "outputId": "1b8d231b-d636-4fbe-d556-746401dcb010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ┌► Эти   det\n",
            "    ┌►└─ типы  nsubj\n",
            "┌─┌─└─┌─ стали \n",
            "│ │   └► есть  xcomp\n",
            "│ │   ┌► в     case\n",
            "│ └──►└─ цеху  obl\n",
            "└──────► .     punct\n",
            "   Пример      \n",
            "┌─ токенизации \n",
            "└► .           punct\n",
            "  ┌► Для          case\n",
            "┌►└─ примера      obl\n",
            "│ ┌► мы           nsubj\n",
            "└─└─ токенезируем \n",
            "    ┌► Научный   amod\n",
            "  ┌►└─ советник  nsubj\n",
            "┌─└─┌─ рассказал \n",
            "│   └► о         advmod\n",
            "└────► ...       punct\n",
            "        ┌► (           punct\n",
            "┌─┌───┌─└─ читать      \n",
            "│ │ ┌─└──► продолжение obj\n",
            "│ │ │   ┌► в           case\n",
            "│ │ └──►└─ источнике   nmod\n",
            "│ └──────► )           punct\n",
            "└────────► .           punct\n",
            "      ┌► В          case\n",
            "    ┌►└─ статье     obl\n",
            "┌─┌─└─── говориться \n",
            "│ │   ┌► о          case\n",
            "│ └──►└─ котиках    obl\n",
            "└──────► .          punct\n",
            "  ┌► Стекло obj\n",
            "┌─└─ бьется \n",
            "└──► .      punct\n"
          ]
        }
      ],
      "source": [
        "from ipymarkup import show_dep_ascii_markup as show_markup\n",
        "for article in data:\n",
        "    text = article.get('text')\n",
        "\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.parse_syntax(syntax_parser)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for sentence in doc.sents: # получаем предложения\n",
        "        sentence.syntax.print()\n",
        "#         for token in sentence.tokens: # получаем токены\n",
        "#             print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2684f184-9807-4fea-92c5-c6ceee30063b",
      "metadata": {
        "id": "2684f184-9807-4fea-92c5-c6ceee30063b"
      },
      "outputs": [],
      "source": [
        "# Продолжение следует..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275291be-52cc-4f56-becc-8a78ed7bf34d",
      "metadata": {
        "id": "275291be-52cc-4f56-becc-8a78ed7bf34d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}